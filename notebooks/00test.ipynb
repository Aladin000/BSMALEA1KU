{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c96a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Numerical features\n",
      "Building tree with 500 samples and 3 features...\n",
      "Categorical features: []\n",
      "Tree built successfully!\n",
      "  R2: 0.9722, Splits: {'x1': 15}\n",
      "Test 2: Categorical features\n",
      "Building tree with 500 samples and 1 features...\n",
      "Categorical features: ['category']\n",
      "Tree built successfully!\n",
      "  R2: 0.9828, Categorical splits: 2\n",
      "Test 3: Mixed features\n",
      "Building tree with 500 samples and 2 features...\n",
      "Categorical features: ['category']\n",
      "Tree built successfully!\n",
      "  R2: 0.9883, Num splits: 22, Cat splits: 2\n",
      "Test 4: Comparison with sklearn\n",
      "  Scratch R2: 0.9722, sklearn R2: 0.9722, Diff: 0.0000\n",
      "Test 5: Prediction count equals leaf count\n",
      "  Unique predictions: 16, Leaves: 16\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor - Correctness Tests\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.decision_tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as SklearnTree\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# TEST 1: Numerical features only\n",
    "# Target = 2*x1 + noise, tree should split on x1\n",
    "print(\"Test 1: Numerical features\")\n",
    "X_num = np.random.randn(n, 3)\n",
    "y_num = 2 * X_num[:, 0] + np.random.randn(n) * 0.3\n",
    "\n",
    "tree_num = DecisionTreeRegressor(max_depth=4, min_samples_leaf=10)\n",
    "tree_num.fit(X_num, y_num, feature_names=['x1', 'x2', 'x3'])\n",
    "y_pred_num = tree_num.predict(X_num)\n",
    "r2_num = 1 - np.sum((y_num - y_pred_num)**2) / np.sum((y_num - np.mean(y_num))**2)\n",
    "\n",
    "print(f\"  R2: {r2_num:.4f}, Splits: {tree_num.get_split_summary()['by_feature']}\")\n",
    "assert r2_num > 0.85 and 'x1' in tree_num.get_split_summary()['by_feature']\n",
    "\n",
    "\n",
    "# TEST 2: Categorical features only\n",
    "# Target depends on category: Low=1, Medium=5, High=10\n",
    "print(\"Test 2: Categorical features\")\n",
    "categories = np.random.choice(['Low', 'Medium', 'High'], size=n)\n",
    "y_cat = np.array([{'Low': 1.0, 'Medium': 5.0, 'High': 10.0}[c] for c in categories])\n",
    "y_cat += np.random.randn(n) * 0.5\n",
    "X_cat = categories.reshape(-1, 1)\n",
    "\n",
    "tree_cat = DecisionTreeRegressor(max_depth=3, min_samples_leaf=10)\n",
    "tree_cat.fit(X_cat, y_cat, feature_names=['category'], categorical_features=['category'])\n",
    "y_pred_cat = tree_cat.predict(X_cat)\n",
    "r2_cat = 1 - np.sum((y_cat - y_pred_cat)**2) / np.sum((y_cat - np.mean(y_cat))**2)\n",
    "\n",
    "print(f\"  R2: {r2_cat:.4f}, Categorical splits: {tree_cat.get_split_summary()['categorical_splits']}\")\n",
    "assert r2_cat > 0.9 and tree_cat.get_split_summary()['categorical_splits'] > 0\n",
    "\n",
    "\n",
    "# TEST 3: Mixed numerical + categorical features\n",
    "# Target = category_effect + 0.5*numerical + noise\n",
    "print(\"Test 3: Mixed features\")\n",
    "num_feat = np.random.randn(n)\n",
    "cat_feat = np.random.choice(['A', 'B', 'C'], size=n)\n",
    "y_mixed = np.array([{'A': 0.0, 'B': 3.0, 'C': 6.0}[c] for c in cat_feat])\n",
    "y_mixed += 0.5 * num_feat + np.random.randn(n) * 0.3\n",
    "X_mixed = np.column_stack([num_feat, cat_feat])\n",
    "\n",
    "tree_mixed = DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)\n",
    "tree_mixed.fit(X_mixed, y_mixed, feature_names=['numerical', 'category'], categorical_features=['category'])\n",
    "y_pred_mixed = tree_mixed.predict(X_mixed)\n",
    "r2_mixed = 1 - np.sum((y_mixed - y_pred_mixed)**2) / np.sum((y_mixed - np.mean(y_mixed))**2)\n",
    "summary = tree_mixed.get_split_summary()\n",
    "\n",
    "print(f\"  R2: {r2_mixed:.4f}, Num splits: {summary['numerical_splits']}, Cat splits: {summary['categorical_splits']}\")\n",
    "assert r2_mixed > 0.9 and summary['numerical_splits'] > 0 and summary['categorical_splits'] > 0\n",
    "\n",
    "\n",
    "# TEST 4: Comparison with sklearn\n",
    "print(\"Test 4: Comparison with sklearn\")\n",
    "tree_sklearn = SklearnTree(max_depth=4, min_samples_leaf=10, random_state=42)\n",
    "tree_sklearn.fit(X_num, y_num)\n",
    "y_pred_sklearn = tree_sklearn.predict(X_num)\n",
    "r2_sklearn = 1 - np.sum((y_num - y_pred_sklearn)**2) / np.sum((y_num - np.mean(y_num))**2)\n",
    "\n",
    "print(f\"  Scratch R2: {r2_num:.4f}, sklearn R2: {r2_sklearn:.4f}, Diff: {abs(r2_num - r2_sklearn):.4f}\")\n",
    "assert abs(r2_num - r2_sklearn) < 0.05\n",
    "\n",
    "\n",
    "# TEST 5: Predictions equal number of leaves\n",
    "print(\"Test 5: Prediction count equals leaf count\")\n",
    "n_unique = len(np.unique(y_pred_num))\n",
    "n_leaves = tree_num.get_n_leaves()\n",
    "print(f\"  Unique predictions: {n_unique}, Leaves: {n_leaves}\")\n",
    "assert n_unique == n_leaves\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe95e1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Forward pass output shape\n",
      "  Input shape: (100, 5), Output shape: (100,)\n",
      "Test 2: Loss decreases during training\n",
      "  Initial loss: 4.6462, Final loss: 0.0471\n",
      "Test 3: Network learns linear relationship\n",
      "  R2 on linear function: 1.0000\n",
      "Test 4: Network learns nonlinear relationship\n",
      "  R2 on sin+cos function: 0.9886\n",
      "Test 5: Comparison with sklearn\n",
      "  Scratch R2: 0.9825, sklearn R2: 0.9908\n",
      "Test 6: MSE loss and derivative\n",
      "  Computed loss: 0.030000, Expected: 0.030000\n",
      "Test 7: Sigmoid activation\n",
      "  R2 with sigmoid: 0.9882\n"
     ]
    }
   ],
   "source": [
    "# Neural Network - Correctness Tests\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "sys.path.append('..')\n",
    "from src.neural_network import NeuralNetwork, DenseLayer, mse_loss, mse_loss_derivative\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Suppress sklearn convergence warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "# Helper to suppress NeuralNetwork verbose output\n",
    "def silent_nn(*args, **kwargs):\n",
    "    with redirect_stdout(io.StringIO()):\n",
    "        nn = NeuralNetwork(*args, **kwargs)\n",
    "    return nn\n",
    "\n",
    "def silent_fit(nn, X, y):\n",
    "    with redirect_stdout(io.StringIO()):\n",
    "        nn.fit(X, y, verbose=0)\n",
    "    return nn\n",
    "\n",
    "# TEST 1: Forward pass produces valid output shape\n",
    "print(\"Test 1: Forward pass output shape\")\n",
    "X_test1 = np.random.randn(100, 5)\n",
    "nn1 = silent_nn(layer_sizes=[5, 10, 1], learning_rate=0.01, epochs=1, random_seed=42)\n",
    "silent_fit(nn1, X_test1, np.random.randn(100))\n",
    "y_pred1 = nn1.predict(X_test1)\n",
    "print(f\"  Input shape: {X_test1.shape}, Output shape: {y_pred1.shape}\")\n",
    "assert y_pred1.shape == (100,), f\"Expected (100,), got {y_pred1.shape}\"\n",
    "\n",
    "# TEST 2: Loss decreases during training\n",
    "print(\"Test 2: Loss decreases during training\")\n",
    "X_train2 = np.random.randn(200, 4)\n",
    "y_train2 = 2 * X_train2[:, 0] + 0.5 * X_train2[:, 1] + np.random.randn(200) * 0.1\n",
    "nn2 = silent_nn(layer_sizes=[4, 16, 8, 1], learning_rate=0.01, epochs=50, random_seed=42)\n",
    "silent_fit(nn2, X_train2, y_train2)\n",
    "initial_loss = nn2.loss_history[0]\n",
    "final_loss = nn2.loss_history[-1]\n",
    "print(f\"  Initial loss: {initial_loss:.4f}, Final loss: {final_loss:.4f}\")\n",
    "assert final_loss < initial_loss, \"Loss should decrease during training\"\n",
    "\n",
    "# TEST 3: Network learns simple linear relationship\n",
    "print(\"Test 3: Network learns linear relationship\")\n",
    "X_linear = np.random.randn(n, 2)\n",
    "y_linear = 3 * X_linear[:, 0] - 2 * X_linear[:, 1]\n",
    "nn3 = silent_nn(layer_sizes=[2, 32, 16, 1], learning_rate=0.01, epochs=200, random_seed=42)\n",
    "silent_fit(nn3, X_linear, y_linear)\n",
    "y_pred3 = nn3.predict(X_linear)\n",
    "r2_linear = 1 - np.sum((y_linear - y_pred3)**2) / np.sum((y_linear - np.mean(y_linear))**2)\n",
    "print(f\"  R2 on linear function: {r2_linear:.4f}\")\n",
    "assert r2_linear > 0.95, f\"Expected R2 > 0.95, got {r2_linear:.4f}\"\n",
    "\n",
    "# TEST 4: Network learns nonlinear relationship\n",
    "print(\"Test 4: Network learns nonlinear relationship\")\n",
    "X_nonlin = np.random.randn(n, 2)\n",
    "y_nonlin = np.sin(X_nonlin[:, 0]) + np.cos(X_nonlin[:, 1])\n",
    "nn4 = silent_nn(layer_sizes=[2, 64, 32, 1], learning_rate=0.01, epochs=300, random_seed=42)\n",
    "silent_fit(nn4, X_nonlin, y_nonlin)\n",
    "y_pred4 = nn4.predict(X_nonlin)\n",
    "r2_nonlin = 1 - np.sum((y_nonlin - y_pred4)**2) / np.sum((y_nonlin - np.mean(y_nonlin))**2)\n",
    "print(f\"  R2 on sin+cos function: {r2_nonlin:.4f}\")\n",
    "assert r2_nonlin > 0.85, f\"Expected R2 > 0.85, got {r2_nonlin:.4f}\"\n",
    "\n",
    "# TEST 5: Comparison with sklearn MLPRegressor\n",
    "print(\"Test 5: Comparison with sklearn\")\n",
    "X_comp = np.random.randn(n, 3)\n",
    "y_comp = X_comp[:, 0]**2 + X_comp[:, 1] + np.random.randn(n) * 0.1\n",
    "\n",
    "nn_scratch = silent_nn(layer_sizes=[3, 32, 16, 1], learning_rate=0.01, epochs=100, random_seed=42)\n",
    "silent_fit(nn_scratch, X_comp, y_comp)\n",
    "y_pred_scratch = nn_scratch.predict(X_comp)\n",
    "r2_scratch = 1 - np.sum((y_comp - y_pred_scratch)**2) / np.sum((y_comp - np.mean(y_comp))**2)\n",
    "\n",
    "sklearn_nn = MLPRegressor(hidden_layer_sizes=(32, 16), activation='relu', solver='sgd',\n",
    "                          learning_rate_init=0.01, max_iter=100, random_state=42)\n",
    "sklearn_nn.fit(X_comp, y_comp)\n",
    "y_pred_sklearn = sklearn_nn.predict(X_comp)\n",
    "r2_sklearn = 1 - np.sum((y_comp - y_pred_sklearn)**2) / np.sum((y_comp - np.mean(y_comp))**2)\n",
    "\n",
    "print(f\"  Scratch R2: {r2_scratch:.4f}, sklearn R2: {r2_sklearn:.4f}\")\n",
    "assert abs(r2_scratch - r2_sklearn) < 0.2, \"Performance should be comparable to sklearn\"\n",
    "\n",
    "# TEST 6: MSE loss and derivative correctness\n",
    "print(\"Test 6: MSE loss and derivative\")\n",
    "y_true = np.array([[1.0], [2.0], [3.0]])\n",
    "y_pred = np.array([[1.1], [2.2], [2.8]])\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "expected_loss = np.mean((y_true - y_pred)**2)\n",
    "print(f\"  Computed loss: {loss:.6f}, Expected: {expected_loss:.6f}\")\n",
    "assert np.isclose(loss, expected_loss), \"MSE loss incorrect\"\n",
    "grad = mse_loss_derivative(y_true, y_pred)\n",
    "expected_grad = 2.0 * (y_pred - y_true)\n",
    "assert np.allclose(grad, expected_grad), \"MSE derivative incorrect\"\n",
    "\n",
    "# TEST 7: Different activations work\n",
    "print(\"Test 7: Sigmoid activation\")\n",
    "nn_sig = silent_nn(layer_sizes=[3, 16, 1], learning_rate=0.1, epochs=100, \n",
    "                   hidden_activation='sigmoid', random_seed=42)\n",
    "silent_fit(nn_sig, X_comp, y_comp)\n",
    "y_pred_sig = nn_sig.predict(X_comp)\n",
    "r2_sig = 1 - np.sum((y_comp - y_pred_sig)**2) / np.sum((y_comp - np.mean(y_comp))**2)\n",
    "print(f\"  R2 with sigmoid: {r2_sig:.4f}\")\n",
    "assert r2_sig > 0.3, \"Sigmoid network should learn something\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TRYG_ML)",
   "language": "python",
   "name": "tryg_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
